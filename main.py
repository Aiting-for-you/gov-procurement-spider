import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from urllib3.exceptions import MaxRetryError
import importlib
import os
import traceback
import argparse
import logging

from province_mapping import get_province_pinyin
from logger_config import get_logger, QueueHandler
from report_generator import create_formatted_report
from url_builder import build_ccgp_search_url

# webdriver-manager is now a dependency
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager


def start_crawl_process(province_pinyin, province_cn, keyword, start_date, end_date, output_dir='output', log_queue=None):
    """
    é‡æ„åçš„ä¸»æµç¨‹ï¼Œè´Ÿè´£å¤„ç†åˆ—è¡¨é¡µæŠ“å–å’Œè¯¦æƒ…é¡µè§£æè°ƒåº¦ã€‚
    """
    # 1. Setup Logger
    logger = get_logger(f"crawler.{province_pinyin}")
    if log_queue:
        logger.addHandler(QueueHandler(log_queue))

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    safe_province_name = province_cn.replace(" ", "_")
    filename = os.path.join(output_dir, f"{safe_province_name}_{keyword}_{start_date}_to_{end_date}.csv")
    
    logger.info(f"å‡†å¤‡å¼€å§‹æŠ“å–: {province_cn} - {keyword}")
    logger.info(f"æ—¥æœŸèŒƒå›´: {start_date} to {end_date}")
    logger.info(f"ç»“æœå°†ä¿å­˜è‡³: {filename}")

    # 2. åŠ¨æ€åŠ è½½çœä»½è§£ææ¨¡å—
    try:
        parser_module = importlib.import_module(f"detail_parsers.{province_pinyin}")
        get_parser_for_url = getattr(parser_module, 'get_parser_for_url')
        get_dynamic_html = getattr(parser_module, 'get_dynamic_html')
    except (ImportError, AttributeError) as e:
        logger.error(f"é”™è¯¯ï¼šæ— æ³•ä¸ºçœä»½ '{province_cn}' åŠ è½½è§£æå™¨æ¨¡å—æˆ–å¿…è¦å‡½æ•°ã€‚")
        logger.error(f"è¯·æ£€æŸ¥ 'detail_parsers/{province_pinyin}.py' æ˜¯å¦ç¬¦åˆè§„èŒƒã€‚")
        logger.error(f"è¯¦ç»†é”™è¯¯: {e}")
        if log_queue: log_queue.put("CRAWL_FAILED")
        return
            
    # 3. åˆå§‹åŒ–Selenium WebDriver
    all_results = []
    driver = None
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        except (WebDriverException, MaxRetryError, ValueError) as e:
            logger.error(f"æ— æ³•å¯åŠ¨WebDriver: {e}")
            logger.error("è¯·ç¡®ä¿Chromeæµè§ˆå™¨å·²æ­£ç¡®å®‰è£…ä¸”ChromeDriverç‰ˆæœ¬å…¼å®¹ã€‚")
            if log_queue: log_queue.put("CRAWL_FAILED")
            return

        # 4. å¾ªç¯æŠ“å–æ‰€æœ‰åˆ—è¡¨é¡µï¼Œè·å–è¯¦æƒ…é¡µé“¾æ¥
        page = 1
        all_detail_links = []
        while True:
            search_url = build_ccgp_search_url(province_cn, start_date, end_date, keyword, page)
            logger.info(f"\nğŸ“„ æ­£åœ¨æŠ“å–åˆ—è¡¨é¡µ ç¬¬ {page} é¡µ...")
            driver.get(search_url)

            try:
                WebDriverWait(driver, 10).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid li a")),
                        EC.presence_of_element_located((By.XPATH, "//*[contains(text(), 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®')]"))
                    )
                )

                if "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®" in driver.page_source:
                    if page == 1:
                        logger.info("ğŸ“­ åœ¨èµ·å§‹é¡µæœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œä»»åŠ¡æå‰ç»“æŸã€‚")
                    else:
                        logger.info("âœ… å·²åˆ°è¾¾ç»“æœæœ«å°¾ï¼Œåˆ—è¡¨æŠ“å–å®Œæˆã€‚")
                    break

                link_elements = driver.find_elements(By.CSS_SELECTOR, ".vT-srch-result-list-bid li a")
                page_links = [link.get_attribute('href') for link in link_elements if link.get_attribute('href')]
                
                if not page_links:
                    logger.info("ğŸ“­ å½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
                    break
                
                all_detail_links.extend(page_links)
                logger.info(f"    æ‰¾åˆ° {len(page_links)} ä¸ªé“¾æ¥ï¼Œç´¯è®¡ {len(all_detail_links)} ä¸ªã€‚")

                next_button = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
                driver.execute_script("arguments[0].click();", next_button)
                page += 1
                time.sleep(2)
            except TimeoutException:
                logger.info("ğŸ“­ é¡µé¢åŠ è½½è¶…æ—¶æˆ–æœªæ‰¾åˆ°ç»“æœåˆ—è¡¨ï¼Œç»“æŸåˆ—è¡¨æŠ“å–ã€‚")
                break
            except NoSuchElementException:
                logger.info("âœ… æ²¡æœ‰'ä¸‹ä¸€é¡µ'æŒ‰é’®ï¼Œåˆ—è¡¨æŠ“å–å®Œæˆã€‚")
                break

        # 5. éå†è¯¦æƒ…é¡µé“¾æ¥ï¼Œè¿›è¡Œè§£æ
        unique_links = sorted(list(set(all_detail_links)), key=lambda x: all_detail_links.index(x))
        logger.info(f"\nğŸ” å¼€å§‹å¤„ç† {len(unique_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥...")
        
        if not unique_links:
             logger.info("ğŸ¤·â€â™€ï¸ æœªæ”¶é›†åˆ°ä»»ä½•è¯¦æƒ…é¡µé“¾æ¥ï¼Œä»»åŠ¡ç»“æŸã€‚")
        
        for i, link in enumerate(unique_links, 1):
            logger.info(f"    ğŸ”— [{i}/{len(unique_links)}] æ­£åœ¨å¤„ç†...")
            parser_instance = get_parser_for_url(link)
            if not parser_instance:
                logger.warning(f"        [è­¦å‘Š] æœªèƒ½ä¸ºé“¾æ¥æ‰¾åˆ°åˆé€‚çš„è§£æå™¨ï¼Œå·²è·³è¿‡ã€‚")
                continue

            html = get_dynamic_html(link)
            if not html:
                logger.warning(f"        [è­¦å‘Š] æœªèƒ½è·å–é¡µé¢å†…å®¹ï¼Œå·²è·³è¿‡ã€‚")
                continue

            try:
                parsed_data = parser_instance.parse(html)
                if parsed_data:
                    for item in parsed_data:
                        item["é“¾æ¥"] = link
                        item["çœä»½"] = province_cn
                    all_results.extend(parsed_data)
                    logger.info(f"        âœ… è§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data)} æ¡è®°å½•ã€‚")
                else:
                    logger.info(f"        [æç¤º] è§£æå™¨è¿”å›ç©ºï¼Œé¡µé¢å¯èƒ½æ— æœ‰æ•ˆä¿¡æ¯ã€‚")
            except Exception as e:
                logger.error(f"        âŒ è§£ææ—¶å‘ç”Ÿé”™è¯¯: {e}")

    except Exception as e:
        logger.error(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e}")
        logger.error(f"è¯¦ç»†å †æ ˆä¿¡æ¯: {traceback.format_exc()}")
        if log_queue: log_queue.put("CRAWL_FAILED")
        return
    finally:
        if driver:
            driver.quit()

    # 6. ä¿å­˜ç»“æœ
    if all_results:
        df = pd.DataFrame(all_results)
        standard_columns = [
            "å‘å¸ƒæ—¥æœŸ", "é¡¹ç›®å·", "é‡‡è´­æ–¹å¼", "é¡¹ç›®åç§°", "ä¾›åº”å•†åç§°",
            "ä¸­æ ‡é‡‘é¢", "åç§°", "å“ç‰Œ", "è§„æ ¼å‹å·", "æ•°é‡", "å•ä»·",
            "é“¾æ¥", "çœä»½"
        ]
        final_columns = [col for col in standard_columns if col in df.columns]
        df = df[final_columns]
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger.info(f"\nğŸ‰ æˆåŠŸæŠ“å– {len(all_results)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜åˆ° {filename}")
        if log_queue: log_queue.put(f"CRAWL_SUCCESS:{filename}")
    else:
        logger.info("\nğŸ¤·â€â™€ï¸ æœ¬æ¬¡ä»»åŠ¡æœªæ‰¾åˆ°ä»»ä½•å¯è§£æçš„æ•°æ®ã€‚")

    if log_queue: log_queue.put("CRAWL_COMPLETE")
    return filename


def main():
    parser = argparse.ArgumentParser(description="æ”¿åºœé‡‡è´­æ•°æ®çˆ¬è™«")
    parser.add_argument("--province", help="çœä»½æ‹¼éŸ³")
    parser.add_argument("--keyword", help="å…³é”®è¯")
    parser.add_argument("--start_date", help="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--end_date", help="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--output", default="output", help="è¾“å‡ºç›®å½•")
    parser.add_argument(
        "--format_report",
        metavar="FILE_PATH",
        nargs='?',
        const=True,
        default=False,
        help="ä»…æ‰§è¡ŒæŠ¥å‘Šæ ¼å¼åŒ–åŠŸèƒ½ã€‚å¯é€‰æ‹©æ€§æä¾›CSVæ–‡ä»¶è·¯å¾„ï¼Œå¦åˆ™å°†åœ¨outputæ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾æœ€æ–°çš„CSVæ–‡ä»¶ã€‚"
    )
    args = parser.parse_args()

    # Setup a general logger for the main script
    cli_logger = get_logger("main_cli")

    if args.format_report:
        # For formatting, we can just use the logger directly.
        # The report generator will use this logger to print messages.
        target_file = args.format_report
        if target_file is True:
            output_dir = 'output'
            files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.csv') and not f.endswith('_report.csv') and not f.endswith('_processed.csv')]
            if not files:
                cli_logger.error("é”™è¯¯ï¼šåœ¨ 'output' ç›®å½•ä¸­æœªæ‰¾åˆ°å¯æ ¼å¼åŒ–çš„CSVæ–‡ä»¶ã€‚")
                exit(1)
            target_file = max(files, key=os.path.getctime)
            cli_logger.info(f"æœªæŒ‡å®šæ–‡ä»¶ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€æ–°çš„æ–‡ä»¶è¿›è¡Œæ ¼å¼åŒ–: {os.path.basename(target_file)}")

        if not os.path.exists(target_file):
            cli_logger.error(f"é”™è¯¯ï¼šæŒ‡å®šçš„æ–‡ä»¶ä¸å­˜åœ¨: {target_file}")
            exit(1)
            
        create_formatted_report(target_file, cli_logger)
        exit(0)

    if not all([args.province, args.keyword, args.start_date, args.end_date]):
        parser.error("æ‰§è¡Œçˆ¬å–ä»»åŠ¡æ—¶ï¼Œå¿…é¡»æä¾› --province, --keyword, --start_date, å’Œ --end_date å‚æ•°ã€‚")

    province_cn = get_province_pinyin(args.province)
    if not province_cn:
        parser.error(f"æ— æ•ˆçš„çœä»½æ‹¼éŸ³: '{args.province}'")

    # When running from CLI, we don't have a queue. The logger will just print to console/file.
    start_crawl_process(
        args.province,
        province_cn,
        args.keyword,
        args.start_date,
        args.end_date,
        args.output,
        log_queue=None 
    )

if __name__ == "__main__":
    main()
