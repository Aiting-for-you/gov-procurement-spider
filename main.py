import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import importlib
import os
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import WebDriverException
from urllib3.exceptions import MaxRetryError
import traceback

from url_builder import build_ccgp_search_url


def get_project_links_from_page(driver, logger):
    project_links = []
    try:
        wait = WebDriverWait(driver, 5) 
        result_container = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid"))
        )
        link_elements = result_container.find_elements(By.CSS_SELECTOR, "li a[href]")

        for element in link_elements:
            href = element.get_attribute("href")
            if href and ("ccgp.gov.cn" in href and (".htm" in href or "detail" in href)):
                project_links.append(href)
    except TimeoutException:
        logger(f"åœ¨5ç§’å†…æœªæ‰¾åˆ°æŒ‡å®šçš„é“¾æ¥å®¹å™¨ï¼Œæ­¤é¡µé¢å¯èƒ½æ— ç»“æœã€‚")
    except Exception as e:
        logger(f"åœ¨å½“å‰é¡µè·å–é¡¹ç›®é“¾æ¥æ—¶å‡ºé”™: {e}")
    return project_links


def start_crawl_process(province_pinyin, province_cn, keyword, start_date, end_date, logger, output_dir='output'):
    """
    é‡æ„åçš„ä¸»æµç¨‹ï¼Œè´Ÿè´£å¤„ç†åˆ—è¡¨é¡µæŠ“å–å’Œè¯¦æƒ…é¡µè§£æè°ƒåº¦ã€‚
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    safe_province_name = province_cn.replace(" ", "_")
    filename = os.path.join(output_dir, f"{safe_province_name}_{keyword}_{start_date}_to_{end_date}.csv")
    
    # ç»Ÿä¸€ä½¿ç”¨ put æ–¹æ³•è®°å½•æ—¥å¿—
    log_func = logger.put if hasattr(logger, 'put') else print

    log_func(f"å‡†å¤‡å¼€å§‹æŠ“å–: {province_cn} - {keyword}")
    log_func(f"æ—¥æœŸèŒƒå›´: {start_date} to {end_date}")
    log_func(f"ç»“æœå°†ä¿å­˜è‡³: {filename}")

    # 1. åŠ¨æ€åŠ è½½çœä»½è§£ææ¨¡å—
    try:
        parser_module = importlib.import_module(f"detail_parsers.{province_pinyin}")
        get_parser_for_url = getattr(parser_module, 'get_parser_for_url')
        get_dynamic_html = getattr(parser_module, 'get_dynamic_html')
    except (ImportError, AttributeError) as e:
        log_func(f"é”™è¯¯ï¼šæ— æ³•ä¸ºçœä»½ '{province_cn}' åŠ è½½è§£æå™¨æ¨¡å—æˆ–å¿…è¦å‡½æ•°ã€‚")
        log_func(f"è¯·æ£€æŸ¥ 'detail_parsers/{province_pinyin}.py' æ˜¯å¦ç¬¦åˆè§„èŒƒã€‚")
        log_func(f"è¯¦ç»†é”™è¯¯: {e}")
        if hasattr(logger, 'put'): logger.put("CRAWL_FAILED")
        return
            
    # 2. åˆå§‹åŒ–Selenium WebDriver
    all_results = []
    driver = None
    try:
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--log-level=3")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_experimental_option('excludeSwitches', ['enable-logging'])
        
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
        except (WebDriverException, MaxRetryError, ValueError) as e:
            log_func(f"æ— æ³•å¯åŠ¨WebDriver: {e}")
            log_func("è¯·ç¡®ä¿Chromeæµè§ˆå™¨å·²æ­£ç¡®å®‰è£…ä¸”ChromeDriverç‰ˆæœ¬å…¼å®¹ã€‚")
            if hasattr(logger, 'put'): logger.put("CRAWL_FAILED")
            return

        # 3. å¾ªç¯æŠ“å–æ‰€æœ‰åˆ—è¡¨é¡µï¼Œè·å–è¯¦æƒ…é¡µé“¾æ¥
        page = 1
        all_detail_links = []
        while True:
            search_url = build_ccgp_search_url(province_cn, start_date, end_date, keyword, page)
            log_func(f"\nğŸ“„ æ­£åœ¨æŠ“å–åˆ—è¡¨é¡µ ç¬¬ {page} é¡µ...")
            # log_func(search_url) # Uncomment for debugging
            driver.get(search_url)

            try:
                # ç­‰å¾…é¡µé¢æ ¸å¿ƒå†…å®¹åŠ è½½
                WebDriverWait(driver, 10).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid li a")),
                        EC.presence_of_element_located((By.XPATH, "//*[contains(text(), 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®')]"))
                    )
                )

                # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰ç»“æœ
                if "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®" in driver.page_source:
                    if page == 1:
                        log_func("ğŸ“­ åœ¨èµ·å§‹é¡µæœªæ‰¾åˆ°ä»»ä½•æ•°æ®ï¼Œä»»åŠ¡æå‰ç»“æŸã€‚")
                    else:
                        log_func("âœ… å·²åˆ°è¾¾ç»“æœæœ«å°¾ï¼Œåˆ—è¡¨æŠ“å–å®Œæˆã€‚")
                    break

                link_elements = driver.find_elements(By.CSS_SELECTOR, ".vT-srch-result-list-bid li a")
                page_links = [link.get_attribute('href') for link in link_elements if link.get_attribute('href')]
                
                if not page_links:
                    log_func("ğŸ“­ å½“å‰é¡µæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå¯èƒ½å·²æ˜¯æœ€åä¸€é¡µã€‚")
                    break
                
                all_detail_links.extend(page_links)
                log_func(f"    æ‰¾åˆ° {len(page_links)} ä¸ªé“¾æ¥ï¼Œç´¯è®¡ {len(all_detail_links)} ä¸ªã€‚")

                # å°è¯•ç‚¹å‡»ä¸‹ä¸€é¡µ
                next_button = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
                driver.execute_script("arguments[0].click();", next_button)
                page += 1
                time.sleep(2) # ç­‰å¾…é¡µé¢è·³è½¬
            except TimeoutException:
                log_func("ğŸ“­ é¡µé¢åŠ è½½è¶…æ—¶æˆ–æœªæ‰¾åˆ°ç»“æœåˆ—è¡¨ï¼Œç»“æŸåˆ—è¡¨æŠ“å–ã€‚")
                break
            except NoSuchElementException:
                log_func("âœ… æ²¡æœ‰'ä¸‹ä¸€é¡µ'æŒ‰é’®ï¼Œåˆ—è¡¨æŠ“å–å®Œæˆã€‚")
                break

        # 4. éå†è¯¦æƒ…é¡µé“¾æ¥ï¼Œè¿›è¡Œè§£æ
        unique_links = sorted(list(set(all_detail_links)), key=lambda x: all_detail_links.index(x))
        log_func(f"\nğŸ” å¼€å§‹å¤„ç† {len(unique_links)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥...")
        
        if not unique_links:
             log_func("ğŸ¤·â€â™€ï¸ æœªæ”¶é›†åˆ°ä»»ä½•è¯¦æƒ…é¡µé“¾æ¥ï¼Œä»»åŠ¡ç»“æŸã€‚")
        
        for i, link in enumerate(unique_links, 1):
            log_func(f"    ğŸ”— [{i}/{len(unique_links)}] æ­£åœ¨å¤„ç†...")
            # log_func(f"    {link}") # Uncomment for debugging

            # a. æ ¹æ®URLè·å–åˆé€‚çš„è§£æå™¨å®ä¾‹
            parser_instance = get_parser_for_url(link)
            if not parser_instance:
                log_func(f"        [è­¦å‘Š] æœªèƒ½ä¸ºé“¾æ¥æ‰¾åˆ°åˆé€‚çš„è§£æå™¨ï¼Œå·²è·³è¿‡ã€‚")
                continue

            # b. è·å–åŠ¨æ€HTML
            html = get_dynamic_html(link)
            if not html:
                log_func(f"        [è­¦å‘Š] æœªèƒ½è·å–é¡µé¢å†…å®¹ï¼Œå·²è·³è¿‡ã€‚")
                continue

            # c. è§£æé¡µé¢
            try:
                parsed_data = parser_instance.parse(html)
                if parsed_data:
                    # d. ç”±ä¸»ç¨‹åºç»Ÿä¸€æ·»åŠ  `é“¾æ¥` å’Œ `çœä»½` å­—æ®µ
                    for item in parsed_data:
                        item["é“¾æ¥"] = link
                        item["çœä»½"] = province_cn
                    all_results.extend(parsed_data)
                    log_func(f"        âœ… è§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data)} æ¡è®°å½•ã€‚")
                else:
                    log_func(f"        [æç¤º] è§£æå™¨è¿”å›ç©ºï¼Œé¡µé¢å¯èƒ½æ— æœ‰æ•ˆä¿¡æ¯ã€‚")
            except Exception as e:
                log_func(f"        âŒ è§£ææ—¶å‘ç”Ÿé”™è¯¯: {e}")
                # log_func(f"        è¯¦ç»†ä¿¡æ¯: {traceback.format_exc()}") # Uncomment for debugging

    except Exception as e:
        log_func(f"æŠ“å–è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥ä¸¥é‡é”™è¯¯: {e}")
        log_func(f"è¯¦ç»†å †æ ˆä¿¡æ¯: {traceback.format_exc()}")
        if hasattr(logger, 'put'): logger.put("CRAWL_FAILED")
        return
    finally:
        if driver:
            driver.quit()

    # 5. ä¿å­˜ç»“æœ
    if all_results:
        df = pd.DataFrame(all_results)
        # æŒ‰ç…§è§„èŒƒä¸­å®šä¹‰çš„æœ€ç»ˆé¡ºåºæ’åˆ—å­—æ®µ
        standard_columns = [
            "å‘å¸ƒæ—¥æœŸ", "é¡¹ç›®å·", "é‡‡è´­æ–¹å¼", "é¡¹ç›®åç§°", "ä¾›åº”å•†åç§°",
            "ä¸­æ ‡é‡‘é¢", "åç§°", "å“ç‰Œ", "è§„æ ¼å‹å·", "æ•°é‡", "å•ä»·",
            "é“¾æ¥", "çœä»½"
        ]
        # è¿‡æ»¤æ‰æ•°æ®ä¸­å¯èƒ½ä¸å­˜åœ¨çš„åˆ—ï¼Œå¹¶ä¿è¯é¡ºåº
        final_columns = [col for col in standard_columns if col in df.columns]
        df = df[final_columns]

        df.to_csv(filename, index=False, encoding='utf-8-sig')
        log_func(f"\nğŸ‰ æˆåŠŸæŠ“å– {len(all_results)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜åˆ° {filename}")
    else:
        log_func("\nğŸ¤·â€â™€ï¸ æœ¬æ¬¡ä»»åŠ¡æœªæ‰¾åˆ°ä»»ä½•å¯è§£æçš„æ•°æ®ã€‚")

    if hasattr(logger, 'put'): logger.put("CRAWL_COMPLETE")


# --- Main execution block for direct script run (testing) ---
if __name__ == '__main__':
    
    class DummyQueue:
        def put(self, message):
            print(message)

    # --- æµ‹è¯•å‚æ•° ---
    # è¯·æ ¹æ®éœ€è¦ä¿®æ”¹ä»¥ä¸‹å‚æ•°è¿›è¡Œæµ‹è¯•
    test_province_pinyin = "chongqing"  # ä¾‹å¦‚: "sichuan"
    test_province_cn = "é‡åº†"      # ä¾‹å¦‚: "å››å·"
    test_keyword = "ä¸­æ ‡"             # ä¾‹å¦‚: "ä¸­æ ‡"
    test_start_date = "2024-05-01" # æ ¼å¼: YYYY-MM-DD
    test_end_date = "2024-05-10"   # æ ¼å¼: YYYY-MM-DD
    test_output_dir = "output_test"
    # ----------------

    print("--- å¼€å§‹ç›´æ¥è¿è¡Œæµ‹è¯• ---")
    start_crawl_process(
        province_pinyin=test_province_pinyin,
        province_cn=test_province_cn,
        keyword=test_keyword,
        start_date=test_start_date,
        end_date=test_end_date,
        logger=DummyQueue(),
        output_dir=test_output_dir
    )
    print("--- æµ‹è¯•è¿è¡Œç»“æŸ ---")
