# main.py

import time
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import importlib
import os

from url_builder import build_ccgp_search_url


def get_project_links_from_page(driver, logger):
    """ä»å½“å‰é¡µé¢è·å–é¡¹ç›®é“¾æ¥"""
    project_links = []

    try:
        wait = WebDriverWait(driver, 10)

        logger(f"é¡µé¢æ ‡é¢˜: {driver.title}")
        logger(f"å½“å‰URL: {driver.current_url}")

        try:
            result_container = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid"))
            )
            logger("æ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨")
            link_elements = result_container.find_elements(By.CSS_SELECTOR, "li a[href]")
            logger(f"åœ¨ç»“æœå®¹å™¨ä¸­æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")

            for element in link_elements:
                href = element.get_attribute("href")
                if href and ("ccgp.gov.cn" in href and (".htm" in href or "detail" in href)):
                    project_links.append(href)
        except TimeoutException:
            logger("æœªæ‰¾åˆ°æ ‡å‡†æœç´¢ç»“æœå®¹å™¨ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨")
            selectors = [
                ".vT-srch-result-list-bid a", "a[href*='ccgp.gov.cn'][href*='.htm']"
            ]
            for selector in selectors:
                elements = driver.find_elements(By.CSS_SELECTOR, selector)
                if elements:
                    logger(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªé“¾æ¥")
                    for element in elements:
                        href = element.get_attribute("href")
                        if href and "ccgp.gov.cn" in href and (".htm" in href or "detail" in href):
                            project_links.append(href)
                    if project_links:
                        break

    except Exception as e:
        logger(f"è·å–é¡¹ç›®é“¾æ¥æ—¶å‡ºé”™: {e}")

    unique_links = list(set(project_links))
    logger(f"æ€»å…±æ‰¾åˆ° {len(unique_links)} ä¸ªå”¯ä¸€é¡¹ç›®é“¾æ¥")
    return unique_links


def start_crawl_process(province_pinyin, province_cn, keyword, start_date, end_date, logger=print):
    """
    çˆ¬è™«ä¸»æµç¨‹
    :param province_pinyin: çœä»½æ‹¼éŸ³ï¼Œç”¨äºåŠ è½½æ¨¡å—
    :param province_cn: çœä»½ä¸­æ–‡åï¼Œç”¨äºæœç´¢å’Œæ–‡ä»¶å
    :param keyword: æœç´¢å…³é”®è¯
    :param start_date: å¼€å§‹æ—¥æœŸ YYYY-MM-DD
    :param end_date: ç»“æŸæ—¥æœŸ YYYY-MM-DD
    :param logger: æ—¥å¿—è®°å½•å‡½æ•°ï¼Œé»˜è®¤ä¸º print
    """
    try:
        parser_module = importlib.import_module(f"detail_parsers.{province_pinyin}")
        get_parser_for_url = getattr(parser_module, 'get_parser_for_url')
        get_dynamic_html = getattr(parser_module, 'get_dynamic_html')
        logger(f"âœ… æˆåŠŸåŠ è½½æ¨¡å—: detail_parsers.{province_pinyin}")
    except (ImportError, AttributeError) as e:
        logger(f"âŒ æ— æ³•åŠ è½½çœä»½ '{province_cn}' çš„è§£ææ¨¡å—: {e}")
        raise

    logger(f'\nğŸ” æ­£åœ¨æŠ“å– {province_cn} åœ°åŒºï¼Œå…³é”®è¯"{keyword}"')
    logger(f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{start_date} ~ {end_date}")

    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=chrome_options)

    all_data = []
    page = 1

    try:
        while True:
            url = build_ccgp_search_url(province_cn, start_date, end_date, keyword, page)
            logger(f"\nğŸ“„ ç¬¬ {page} é¡µï¼š{url}")
            driver.get(url)

            links = get_project_links_from_page(driver, logger)
            if not links:
                logger("ğŸ“­ å½“å‰é¡µæ— æœ‰æ•ˆé¡¹ç›®é“¾æ¥ï¼Œç»“æŸæŠ“å–")
                break

            for i, link in enumerate(links, 1):
                logger(f"ğŸ”— [{i}/{len(links)}] æŠ“å–è¯¦æƒ…: {link[:80]}...")
                try:
                    parser = get_parser_for_url(link)
                    if not parser:
                        logger(f"    [è­¦å‘Š] æœªæ‰¾åˆ°è§£æå™¨ï¼Œå·²è·³è¿‡ã€‚")
                        continue
                    
                    parser_type = 'local' if "dfgg" in link else 'central'
                    detail_html = get_dynamic_html(link, parser_type=parser_type)
                    
                    if not detail_html:
                        logger(f"    [è­¦å‘Š] æœªè·å–åˆ°HTMLå†…å®¹ï¼Œå·²è·³è¿‡ã€‚")
                        continue
                        
                    parsed_data_list = parser.parse(detail_html)
                    if parsed_data_list:
                        logger(f"    âœ… è§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data_list)} æ¡è®°å½•ã€‚")
                        for data_dict in parsed_data_list:
                            data_dict.update({"é“¾æ¥": link, "çœä»½": province_cn})
                        all_data.extend(parsed_data_list)
                    else:
                        logger(f"    [è­¦å‘Š] è§£æå™¨æœªèƒ½æå–åˆ°æ•°æ®ã€‚")

                except Exception as e:
                    logger(f"    âŒ å¤„ç†é“¾æ¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")

            try:
                next_button = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
                driver.execute_script("arguments[0].click();", next_button)
                page += 1
                time.sleep(2)
            except:
                logger("âœ… æ²¡æœ‰æ›´å¤šé¡µé¢")
                break
    finally:
        driver.quit()

    if all_data:
        # Create output dir if not exists
        if not os.path.exists('output'):
            os.makedirs('output')
        
        df = pd.DataFrame(all_data)
        filename = f"output/ä¸­æ ‡å…¬å‘Š_{keyword}_{province_cn}_{start_date}_to_{end_date}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        logger(f"\nâœ… æˆåŠŸæŠ“å– {len(all_data)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜è‡³ï¼š{filename}")
    else:
        logger("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®")


def main_cli():
    """å‘½ä»¤è¡Œäº¤äº’ç‰ˆæœ¬çš„ä¸»å‡½æ•°"""
    print("ğŸ“Œ æ¬¢è¿ä½¿ç”¨ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™« (å‘½ä»¤è¡Œç‰ˆ)")

    # åŠ¨æ€è·å–çœä»½åˆ—è¡¨
    parsers_dir = os.path.join(os.path.dirname(__file__), 'detail_parsers')
    province_files = [f.replace('.py', '') for f in os.listdir(parsers_dir) if f.endswith('.py') and not f.startswith('__') and f != 'base.py' and f != 'test.py']
    
    # å»ºç«‹ä¸€ä¸ªç®€æ˜“çš„ä¸­æ–‡åˆ°æ‹¼éŸ³çš„æ˜ å°„
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ‰‹åŠ¨ç»´æŠ¤ï¼Œæˆ–è€…ä»ä¸€ä¸ªæ›´å¯é çš„æ•°æ®æºç”Ÿæˆ
    province_cn_map = {
        'anhui': 'å®‰å¾½', 'chongqing': 'é‡åº†', 'guangdong': 'å¹¿ä¸œ', 'guangxi': 'å¹¿è¥¿', 
        'hebei': 'æ²³åŒ—', 'hubei': 'æ¹–åŒ—', 'jiangsu': 'æ±Ÿè‹', 'shandong': 'å±±ä¸œ', 
        'sichuan': 'å››å·', 'zhejiang': 'æµ™æ±Ÿ' 
        # å…¶ä»–çœä»½...
    }
    supported_provinces_cn = [province_cn_map.get(p, p) for p in sorted(province_files)]
    pinyin_cn_map = {v: k for k, v in province_cn_map.items()}


    print(f"æ”¯æŒçš„çœä»½: {', '.join(supported_provinces_cn)}")
    province_cn_input = input("è¯·è¾“å…¥çœä»½ä¸­æ–‡å (ä¾‹å¦‚: æ±Ÿè‹): ").strip()
    
    province_pinyin_input = pinyin_cn_map.get(province_cn_input)

    if not province_pinyin_input:
        print(f"é”™è¯¯ï¼šä¸æ”¯æŒçš„çœä»½ '{province_cn_input}'ã€‚")
        return
        
    keyword = input("è¯·è¾“å…¥å…³é”®è¯ (ä¾‹å¦‚: ç©ºè°ƒ): ").strip()
    start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (YYYY-MM-DD): ").strip()
    end_date = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ (YYYY-MM-DD): ").strip()

    if not all([province_pinyin_input, keyword, start_date, end_date]):
        print("é”™è¯¯ï¼šæ‰€æœ‰è¾“å…¥é¡¹éƒ½ä¸èƒ½ä¸ºç©ºã€‚")
        return

    start_crawl_process(province_pinyin_input, province_cn_input, keyword, start_date, end_date)


if __name__ == "__main__":
    main_cli()