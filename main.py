# main.py

import time
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import importlib

from url_builder import build_ccgp_search_url


def get_project_links_from_page(driver):
    """ä»å½“å‰é¡µé¢è·å–é¡¹ç›®é“¾æ¥"""
    project_links = []

    try:
        wait = WebDriverWait(driver, 10)

        print(f"é¡µé¢æ ‡é¢˜: {driver.title}")
        print(f"å½“å‰URL: {driver.current_url}")

        try:
            result_container = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid"))
            )
            print("æ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨")
            link_elements = result_container.find_elements(By.CSS_SELECTOR, "li a[href]")
            print(f"åœ¨ç»“æœå®¹å™¨ä¸­æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")

            for element in link_elements:
                href = element.get_attribute("href")
                if href and ("ccgp.gov.cn" in href and (".htm" in href or "detail" in href)):
                    project_links.append(href)
                    print(f"æ·»åŠ é¡¹ç›®é“¾æ¥: {href[:80]}...")

        except TimeoutException:
            print("æœªæ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
            selectors = [
                "a[style='line-height:18px']",
                ".vT-srch-result-list-bid a",
                "a[href*='ccgp.gov.cn'][href*='.htm']",
                "a[href*='cggg']",
                ".vT-srch-result a",
                "li a[href]",
                "a[title]"
            ]

            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªé“¾æ¥")
                        for element in elements:
                            href = element.get_attribute("href")
                            if href and "ccgp.gov.cn" in href and (".htm" in href or "detail" in href):
                                project_links.append(href)
                                print(f"æ·»åŠ é“¾æ¥: {href[:80]}...")
                        if project_links:
                            print(f"æˆåŠŸè·å–åˆ° {len(project_links)} ä¸ªé¡¹ç›®é“¾æ¥")
                            break
                except Exception as e:
                    print(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue

    except Exception as e:
        print(f"è·å–é¡¹ç›®é“¾æ¥æ—¶å‡ºé”™: {e}")

    unique_links = list(set(project_links))
    print(f"æ€»å…±æ‰¾åˆ° {len(unique_links)} ä¸ªå”¯ä¸€é¡¹ç›®é“¾æ¥")

    if not unique_links:
        print("è°ƒè¯•ä¿¡æ¯: é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥")
        all_links = driver.find_elements(By.TAG_NAME, "a")
        for i, link in enumerate(all_links[:10]):
            href = link.get_attribute("href")
            text = link.text.strip()
            print(f"  é“¾æ¥ {i+1}: {href} - æ–‡æœ¬: {text[:30]}...")

    return unique_links


def main():
    print("ğŸ“Œ æ¬¢è¿ä½¿ç”¨ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«")

    province_map = {
        "é‡åº†": "chongqing",
        "æ±Ÿè‹": "jiangsu",
        "å¹¿ä¸œ": "guangdong",
        "å±±ä¸œ": "shandong",
        "æ¹–åŒ—": "hubei",
        "æµ™æ±Ÿ": "zhejiang"

    province_name = input(f"è¯·è¾“å…¥çœä»½ (æ”¯æŒ: {', '.join(province_map.keys())}): ").strip()
    if province_name not in province_map:
        print(f"é”™è¯¯ï¼šä¸æ”¯æŒçš„çœä»½ '{province_name}'ã€‚")
        return
        
    province_pinyin = province_map[province_name]

    start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼š").strip()
    end_date = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼š").strip()
    keyword = "ç©ºè°ƒ"

    try:
        parser_module = importlib.import_module(f"detail_parsers.{province_pinyin}")
        get_parser_for_url = getattr(parser_module, 'get_parser_for_url')
        get_dynamic_html = getattr(parser_module, 'get_dynamic_html')
        print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å—: detail_parsers.{province_pinyin}")
    except (ImportError, AttributeError) as e:
        print(f"âŒ æ— æ³•åŠ è½½çœä»½ '{province_name}' çš„è§£ææ¨¡å—: {e}")
        return

    print("\nğŸ” æ­£åœ¨æŠ“å– " + province_name + " åœ°åŒºï¼Œå…³é”®è¯" + keyword + " ä¸­æ ‡å…¬å‘Š")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{start_date} ~ {end_date}")

    chrome_options = Options()
    # å¼€å¯å¯è§†åŒ–æµè§ˆå™¨è°ƒè¯•
    # chrome_options.add_argument("--headless")
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')

    driver = webdriver.Chrome(options=chrome_options)

    all_data = []
    page = 1

    while True:
        url = build_ccgp_search_url(province_name, start_date, end_date, keyword, page)
        print(f"\nğŸ“„ ç¬¬ {page} é¡µï¼š{url}")
        driver.get(url)

        links = get_project_links_from_page(driver)
        if not links:
            print("ğŸ“­ å½“å‰é¡µæ— æœ‰æ•ˆé¡¹ç›®é“¾æ¥ï¼Œç»“æŸæŠ“å–")
            break

        for i, link in enumerate(links, 1):
            print(f"ğŸ”— [{i}/{len(links)}] æ­£åœ¨æŠ“å–è¯¦æƒ…é¡µ: {link}")
            try:
                parser = get_parser_for_url(link)
                if not parser:
                    print(f"    [è­¦å‘Š] æ— æ³•ä¸ºé“¾æ¥æ‰¾åˆ°åˆé€‚çš„è§£æå™¨ï¼Œå·²è·³è¿‡ã€‚")
                    continue
                
                parser_type = 'local' if "dfgg" in link else 'central'
                detail_html = get_dynamic_html(link, parser_type=parser_type)
                
                if not detail_html:
                    print(f"    [è­¦å‘Š] æ— æ³•è·å–é“¾æ¥çš„HTMLå†…å®¹ï¼Œå·²è·³è¿‡ã€‚")
                    continue
                    
                parsed_data_list = parser.parse(detail_html)
                if parsed_data_list:
                    print(f"    âœ… è§£ææˆåŠŸï¼Œè·å¾— {len(parsed_data_list)} æ¡è®°å½•ã€‚")
                    # Add common info to each record
                    for data_dict in parsed_data_list:
                        data_dict.update({
                            "é“¾æ¥": link,
                            "çœä»½": province_name
                        })
                    all_data.extend(parsed_data_list)
                else:
                    print(f"    [è­¦å‘Š] è§£æå™¨æœªèƒ½ä»æ­¤é“¾æ¥æå–åˆ°æ•°æ®ã€‚")

            except Exception as e:
                print(f"    âŒ å¤„ç†é“¾æ¥ {link} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

        # å°è¯•ç‚¹å‡»"ä¸‹ä¸€é¡µ"
        try:
            next_button = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
            driver.execute_script("arguments[0].click();", next_button)
            page += 1
            time.sleep(2)
        except:
            print("âœ… æ²¡æœ‰æ›´å¤šé¡µé¢")
            break

    driver.quit()

    if all_data:
        df = pd.DataFrame(all_data)
        filename = f"output/ä¸­æ ‡å…¬å‘Š_{keyword}_{province_name}_{start_date}_to_{end_date}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nâœ… æˆåŠŸæŠ“å– {len(all_data)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜è‡³ï¼š{filename}")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®")


if __name__ == "__main__":
    main()
