# main.py

import argparse
import time
import pandas as pd
from datetime import datetime

from simulate_search import simulate_search
from detail_parsers import PARSER_MAP
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def extract_details(results, province):
    """å¤„ç†æ¯ä¸ªè¯¦æƒ…é¡µï¼Œä½¿ç”¨å¯¹åº”çœä»½è§£æå™¨æå–å­—æ®µ"""
    all_data = []
    parser_class = PARSER_MAP.get(province)
    if not parser_class:
        print(f"âŒ æš‚æ—  {province} çš„è§£æå™¨")
        return []

    parser = parser_class()

    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=chrome_options)

    for i, item in enumerate(results, 1):
        print(f"ğŸ” æ­£åœ¨å¤„ç†ç¬¬ {i} æ¡ï¼š{item['æ ‡é¢˜']}")
        try:
            driver.get(item["é“¾æ¥"])
            time.sleep(2)
            html = driver.page_source
            data = parser.parse(html)
            data.update({
                "æ ‡é¢˜": item["æ ‡é¢˜"],
                "é“¾æ¥": item["é“¾æ¥"],
                "å‘å¸ƒæ—¥æœŸ": item["å‘å¸ƒæ—¥æœŸ"],
                "çœä»½": province
            })
            all_data.append(data)
        except Exception as e:
            print(f"â— è§£æå¤±è´¥ï¼š{e}ï¼Œé“¾æ¥ï¼š{item['é“¾æ¥']}")
            continue

    driver.quit()
    return all_data

def save_to_csv(data, keyword, province, start_date, end_date):
    """ä¿å­˜ä¸º CSV æ–‡ä»¶"""
    df = pd.DataFrame(data)
    filename = f"output/ä¸­æ ‡å…¬å‘Š_{keyword}_{province}_{start_date}_to_{end_date}.csv"
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"\nâœ… æ•°æ®å·²ä¿å­˜ï¼š{filename}")

def main():
    parser = argparse.ArgumentParser(description="ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«")
    parser.add_argument("--province", required=True, help="çœä»½ï¼ˆå¦‚ï¼šæ±Ÿè‹ï¼‰")
    parser.add_argument("--keyword", default="ç©ºè°ƒ", help="å…³é”®è¯")
    parser.add_argument("--start", required=True, help="å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰")
    parser.add_argument("--end", required=True, help="ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰")
    parser.add_argument("--pages", type=int, default=3, help="æœ€å¤§é¡µæ•°")

    args = parser.parse_args()

    province = args.province
    keyword = args.keyword
    start_date = args.start
    end_date = args.end
    max_pages = args.pages

    print(f"\nğŸ“ çœä»½ï¼š{province}")
    print(f"ğŸ” å…³é”®è¯ï¼š{keyword}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{start_date} ~ {end_date}")
    print(f"ğŸ“„ æŠ“å–é¡µæ•°ï¼š{max_pages}")

    print("\nğŸ§­ å¼€å§‹æ¨¡æ‹Ÿç­›é€‰å¹¶è·å–æœç´¢ç»“æœ...")
    search_results = simulate_search(keyword, start_date, end_date, province, max_pages=max_pages)

    if not search_results:
        print("âŒ æœªè·å–åˆ°æœç´¢ç»“æœ")
        return

    print(f"\nğŸ“¦ å…±è·å–åˆ° {len(search_results)} æ¡æœç´¢ç»“æœï¼Œå¼€å§‹æå–è¯¦æƒ…é¡µå†…å®¹...")
    all_data = extract_details(search_results, province)

    if all_data:
        save_to_csv(all_data, keyword, province, start_date, end_date)
    else:
        print("âŒ æ— æœ‰æ•ˆæ•°æ®æå–")

if __name__ == "__main__":
    main()
