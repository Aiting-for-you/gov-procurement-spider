# main.py

import time
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

from url_builder import build_ccgp_search_url
from detail_parsers import PARSER_MAP


def get_project_links_from_page(driver):
    """ä»å½“å‰é¡µé¢è·å–é¡¹ç›®é“¾æ¥"""
    project_links = []

    try:
        wait = WebDriverWait(driver, 10)

        print(f"é¡µé¢æ ‡é¢˜: {driver.title}")
        print(f"å½“å‰URL: {driver.current_url}")

        try:
            result_container = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".vT-srch-result-list-bid"))
            )
            print("æ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨")
            link_elements = result_container.find_elements(By.CSS_SELECTOR, "li a[href]")
            print(f"åœ¨ç»“æœå®¹å™¨ä¸­æ‰¾åˆ° {len(link_elements)} ä¸ªé“¾æ¥")

            for element in link_elements:
                href = element.get_attribute("href")
                if href and ("ccgp.gov.cn" in href and (".htm" in href or "detail" in href)):
                    project_links.append(href)
                    print(f"æ·»åŠ é¡¹ç›®é“¾æ¥: {href[:80]}...")

        except TimeoutException:
            print("æœªæ‰¾åˆ°æœç´¢ç»“æœå®¹å™¨ï¼Œå°è¯•å…¶ä»–æ–¹æ³•")
            selectors = [
                "a[style='line-height:18px']",
                ".vT-srch-result-list-bid a",
                "a[href*='ccgp.gov.cn'][href*='.htm']",
                "a[href*='cggg']",
                ".vT-srch-result a",
                "li a[href]",
                "a[title]"
            ]

            for selector in selectors:
                try:
                    elements = driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        print(f"ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(elements)} ä¸ªé“¾æ¥")
                        for element in elements:
                            href = element.get_attribute("href")
                            if href and "ccgp.gov.cn" in href and (".htm" in href or "detail" in href):
                                project_links.append(href)
                                print(f"æ·»åŠ é“¾æ¥: {href[:80]}...")
                        if project_links:
                            print(f"æˆåŠŸè·å–åˆ° {len(project_links)} ä¸ªé¡¹ç›®é“¾æ¥")
                            break
                except Exception as e:
                    print(f"é€‰æ‹©å™¨ '{selector}' å¤±è´¥: {e}")
                    continue

    except Exception as e:
        print(f"è·å–é¡¹ç›®é“¾æ¥æ—¶å‡ºé”™: {e}")

    unique_links = list(set(project_links))
    print(f"æ€»å…±æ‰¾åˆ° {len(unique_links)} ä¸ªå”¯ä¸€é¡¹ç›®é“¾æ¥")

    if not unique_links:
        print("è°ƒè¯•ä¿¡æ¯: é¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥")
        all_links = driver.find_elements(By.TAG_NAME, "a")
        for i, link in enumerate(all_links[:10]):
            href = link.get_attribute("href")
            text = link.text.strip()
            print(f"  é“¾æ¥ {i+1}: {href} - æ–‡æœ¬: {text[:30]}...")

    return unique_links


def extract_detail(driver, link, province):
    parser_class = PARSER_MAP.get(province)
    if not parser_class:
        print(f"âŒ æ²¡æœ‰ {province} çš„è§£æå™¨")
        return None
    parser = parser_class()
    try:
        driver.get(link)
        time.sleep(2)
        html = driver.page_source
        data = parser.parse(html)
        data.update({
            "é“¾æ¥": link,
            "çœä»½": province
        })
        return data
    except Exception as e:
        print(f"âš ï¸ è§£æå¤±è´¥: {e}")
        return None


def main():
    print("ğŸ“Œ æ¬¢è¿ä½¿ç”¨ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«")
    province = input("è¯·è¾“å…¥çœä»½ï¼ˆå¦‚ æ±Ÿè‹ï¼‰ï¼š").strip()
    start_date = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼š").strip()
    end_date = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸï¼ˆYYYY-MM-DDï¼‰ï¼š").strip()
    keyword = "ç©ºè°ƒ"

    print(f"\nğŸ” æ­£åœ¨æŠ“å– {province} åœ°åŒºï¼Œå…³é”®è¯â€œ{keyword}â€ ä¸­æ ‡å…¬å‘Š")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´ï¼š{start_date} ~ {end_date}")

    chrome_options = Options()
    # å¼€å¯å¯è§†åŒ–æµè§ˆå™¨è°ƒè¯•
    # chrome_options.add_argument("--headless")
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--no-sandbox')

    driver = webdriver.Chrome(options=chrome_options)

    all_data = []
    page = 1

    while True:
        url = build_ccgp_search_url(province, start_date, end_date, keyword, page)
        print(f"\nğŸ“„ ç¬¬ {page} é¡µï¼š{url}")
        driver.get(url)

        links = get_project_links_from_page(driver)
        if not links:
            print("ğŸ“­ å½“å‰é¡µæ— æœ‰æ•ˆé¡¹ç›®é“¾æ¥ï¼Œç»“æŸæŠ“å–")
            break

        for i, link in enumerate(links, 1):
            print(f"ğŸ”— [{i}/{len(links)}] æ­£åœ¨æŠ“å–è¯¦æƒ…é¡µ: {link}")
            data = extract_detail(driver, link, province)
            if data:
                all_data.append(data)

        # å°è¯•ç‚¹å‡»â€œä¸‹ä¸€é¡µâ€
        try:
            next_button = driver.find_element(By.LINK_TEXT, "ä¸‹ä¸€é¡µ")
            driver.execute_script("arguments[0].click();", next_button)
            page += 1
            time.sleep(2)
        except:
            print("âœ… æ²¡æœ‰æ›´å¤šé¡µé¢")
            break

    driver.quit()

    if all_data:
        df = pd.DataFrame(all_data)
        filename = f"output/ä¸­æ ‡å…¬å‘Š_{keyword}_{province}_{start_date}_to_{end_date}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"\nâœ… æˆåŠŸæŠ“å– {len(all_data)} æ¡æ•°æ®ï¼Œå·²ä¿å­˜è‡³ï¼š{filename}")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ•°æ®")


if __name__ == "__main__":
    main()
